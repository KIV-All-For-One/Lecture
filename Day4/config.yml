# train_from: en-fr_step_50000.pt

## Where the samples will be written
save_data: data
## Where the vocab(s) will be written
src_vocab: vocab/vocab.ko
tgt_vocab: vocab/vocab.en
# Prevent overwriting existing files in the folder
overwrite: False
keep_checkpoint: 4
overwrite: False
early_stopping: 3

# Corpus opts:
data:
    corpus_1:
        path_src: train.ko.encoded
        path_tgt: train.en.encoded
    valid:
        path_src: val.ko.encoded
        path_tgt: val.en.encoded
        
# General opts
save_model: ko-en
save_checkpoint_steps: 10000
valid_steps: 10000
train_steps: 200000

# Batching
queue_size: 10000
bucket_size: 32768
world_size: 1
gpu_ranks: -0
batch_type: "tokens"

# 전임자
batch_size: 2048
accum_count: [4]
# judy
# 배치사이즈 및 accum count 를 절반 단위로 줄이면 학습 속도가 기본 단위당 2~30초 -> 8초로 대폭 단축.
# 모델 성능은 많이 떨어지므로 실서비스 모델에는 사용하지 말 것.
# batch_size: 512
# accum_count: [1]

batch_type: "tokens"
valid_batch_size: 8
# valid_batch_size: 1
max_generator_batches: 0
accum_steps: [0]

# Optimization
model_dtype: "fp32"
optim: "adam"
learning_rate: 2
warmup_steps: 8000
decay_method: "noam"
adam_beta2: 0.998
max_grad_norm: 0
label_smoothing: 0.1
param_init: 0
param_init_glorot: true
normalization: "tokens"

# Model
encoder_type: transformer
decoder_type: transformer
position_encoding: true
enc_layers: 6
dec_layers: 6
heads: 8
rnn_size: 512
word_vec_size: 512
transformer_ff: 2048
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]